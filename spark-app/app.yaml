apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: my-spark-job
  namespace: default
spec:
  volumes:
    - name: store
      emptyDir: {}
  sparkConf:
    spark.jars.packages: org.apache.hadoop:hadoop-aws:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0
    spark.driver.extraJavaOptions: '-Divy.cache.dir=/tmp -Divy.home=/tmp'
    spark.kubernetes.allocation.batch.size: '10'
  hadoopConf:
    fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
    fs.s3a.aws.credentials.provider: com.amazonaws.auth.InstanceProfileCredentialsProvider
  type: Python
  pythonVersion: '3'
  mode: cluster
  image: "gcr.io/spark-operator/spark:v3.1.1"
  imagePullPolicy: IfNotPresent
  mainApplicationFile: s3a://my-pyspark-bucket/pyspark-job-76d2e-1692305903.py
  sparkVersion: 3.1.1
  restartPolicy:
    type: Never
  driver:
    cores: 1
    coreLimit: 1200m
    memory: 512m
    labels:
      version: 3.1.1
    serviceAccount: spark
    volumeMounts:
      - name: store
        mountPath: /tmp
  executor:
    cores: 1
    instances: 1
    memory: 512m
    labels:
      version: 3.1.1
    volumeMounts:
      - name: store
        mountPath: /tmp
