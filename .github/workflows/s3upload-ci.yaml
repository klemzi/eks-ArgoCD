name: s3 upload ci

on:
  push:
    branches:
      - main
    paths:
      - spark-job/**

env:
  BUCKET: my-pyspark-bucket
  FILE_PREFIX: pyspark-job

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains 2 jobs called "check and prod"
  build:
    # The type of runner that the job will run on
    name: build
    runs-on: ubuntu-latest
    outputs:
      TAG: ${{ steps.tag.outputs.TAG }}
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v3

      - name: Configure AWS Credentials Action For GitHub Actions
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Generate file version tag
        id: tag
        run: |
          sha=${GITHUB_SHA::5}
          timestamp=$(date +%s)
          echo "TAG=${sha}-${timestamp}" >> "$GITHUB_OUTPUT"

      - name: rename spark job file
        run: |
          mv spark-job/main.py spark-job/${{ env.FILE_PREFIX }}-${{ steps.tag.outputs.TAG }}.py
      
      - name: upload to s3
        run: |
          aws s3 cp spark-job/${{ env.FILE_PREFIX }}-${{ steps.tag.outputs.TAG }}.py s3://${{ env.BUCKET }}

  commit:
    name: commit
    runs-on: ubuntu-latest
    needs: build
    permissions:
      contents: write
    env:
      TAG: ${{ needs.build.outputs.TAG }}

    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v3

      - name: Update values.yaml
        uses: fjogeleit/yaml-update-action@main
        with:
          valueFile: 'spark-app/app.yaml'
          propertyPath: 'spec.mainApplicationFile'
          branch: main
          value: s3a://${{ env.BUCKET }}/${{ env.FILE_PREFIX }}-${{ env.TAG }}.py
          commitChange: true
          message: 'ci updated s3 location'
